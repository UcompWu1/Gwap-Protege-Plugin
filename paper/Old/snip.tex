Table~\ref{table:eval_qual} presents inter-rater agreement per task and per setting. 
The number of raters per task is given in parentheses.
According to the interpretation of Landis and Koch~\cite{landis1977}
the inter-rater agreement among manual expert evaluators (Setting 1) is moderate. Agreement among the four groups of CrowdFlower workers is 
substantial. The combined agreement (manual expert and crowdworkers) is always higher than for manual evalators alone.
%, which indicates that crowdworkers have a high level of agreement with the average opinion of manual evaluators.
A detailed inspection of results reveals that judgement is difficult on some questions, for example 
relevance of given concepts for the climate change domain often depends on the point of view and granularity of the domain model.
But in general, crowdworkers have a tendency to higher inter-rater agreement, which often corresponds with the majority opinion of manual experts,
thereby raising Fleiss' kappa (Setting combined).
