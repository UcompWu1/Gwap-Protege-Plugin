
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\usepackage{url}
\urldef{\mailsa}\path|{florian.hanika, gerhard.wohlgenannt|
\urldef{\mailsb}\path|marta.sabou|
\urldef{\mailsc}\path|erika.siebert-cole, peter.strasser, lncs}@springer.com|    
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter % start of an individual contribution

% first the title is needed
\title{Crowdsourcing Enabled Ontology Engineering}
%Alternatives
%* Crowd-based ontology engineering
%* Tool support for crowd-based ontology engineering
%* The uComp Prot\'eg\'e Plugin: Towards Embedded Human Computation for Ontology Engineering
%* Enabling Embedded Human Computation in Ontology Engineering with the uComp Prot\'eg\'e Plugin

% a short form should be given in case it is too long for the running head
\titlerunning{Crowdsourcing Enabled Ontology Engineering}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Florian Hanika
\and Gerhard Wohlgennant
\and Marta Sabou}
\author{} % for initial submission
%
\authorrunning{Hanika et al.}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published

\institute{WU\
Vienna\\
\mailsa\\
\url{http://www.wu.ac.at}}

\institute{MODUL Univerisity\
Vienna\\
\mailsb\\
\url{http://www.wu.ac.at}}

\institute{}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
Recent years have seen an increase in the use of crowdsourcing-based methods at various stages of the ontology engineering lifecycle (e.g., verification of subsumption, assigning equivalences between concepts etc) thus laying the foundations of a novel approach to ontology engineering (OE). Take up of this early research by the community at large, especially by practitioners, is however currently hampered by 1) a lack of understanding of which stages of the ontology engineering process can be crowdsourced and 2) tool support in ontology engineering platforms that would allow easy insertion of crowdsourcing into ontology engineering workflows. In this paper we perform an overview of recent works in the area and take a scenario-based approach to identifying those stages of the OE process where crowdsourcing makes sense. Then, we present the “uComp Prot\'eg\'e plugin”, a plugin for the popular Prot\'eg\'e ontology engineering platform that facilitates the integration of crowdsourcing stages into the OE process. TBD: clarify novelty (e.g., which new tasks we introduce?), sum up some important evaluation results.
\keywords{human computation, crowdsourcing, ontology engineering, ontology learning, Prot\'eg\'e plugin}
\end{abstract}


\section{Introduction}

%[TBD - what is ontology engineering - and how it is always more distributed - WebProt\'eg\'e, GATE TEamWare - to create semantic annotations on text]

Ontology engineering, as the process involving the creation and maintenance of ontologies during their entire life-cycle, is a crucial component of any Semantic Web based system. Traditionally performed by a small group of ontology experts, ontology engineering tends to be a complex but, above all, time-consuming process. In an attempt to solve this situation, the Prot\'eg\'e ontology editor has been therefore redesigned to open up the ontology engineering process to a larger, distributed group of contributors and enable collective knowledge creation, through WebProt\'eg\'e~\cite{Tudorache2013}. Similarly, in the area of natural language processing, GATE Teamware extends the GATE linguistic annotation toolkit with distributed knowledge creation capabilities~\cite{Bontcheva2013}. While these extensions primarily support the collaborative and distributed work of knowledge experts (both ontology engineers and linguists), an increasing trend is allowing large populations of \textit{non-experts} to create knowledge through the use of Human Computation platforms such as games or mechanised labour platforms.

%Crowdsourcing techniques allow outsourcing a task to ``an undefined, generally large group of people in the form of an open call"~\cite{Howe2009} and are usually classified in three major genres depending on the motivation of the human contributors (e.g., payment vs. fun vs. altruism). 

Human Computation (HC) describes systems that combine automated methods with large numbers of human contributors who collaborate to perform a task, typically involving an automated system that serves up task instances and then aggregates the produced results ~\cite{Quinn2011}. HC systems are usually classified in three major genres depending on the motivation of the human contributors (e.g., payment vs. fun vs. altruism).
% HC tasks usually have an artificial intelligence flavour, with the humans being needed to perform a sub-task for which effective algorithms are not yet available. HC includes a number of genres, which can be distinguished along various dimensions, such as the motivation of human contributors (e.g., entertainment, altruism, payment) and skills required, how individual results are aggregated and how quality is managed [35]. Key HC genres include crowdsourcing, mechanised labour (e.g. Amazon Mechanical Turk),  and games with a purpose (GWAP), where human contributors are motivated by formulating tasks as enjoyable games [21]. 
 Mechanised labour (MLab) is a type of paid-for crowdsourcing, where contributors choose to carry out small tasks (or micro-tasks) and are paid a small amount of money in return (often referred to as micro-payments). The most popular platform for mechanised labour is Amazon`s Mechanical Turk (MTurk) which allows requesters to post their micro-tasks in the form of Human Intelligence Tasks (or HITs) to a large population of micro-workers (often referred to as “turkers”). Most projects use crowdsourcing marketplaces such as MTurk and CrowdFlower (CF), where contributors are extrinsically motivated through economic incentives. Games with a purpose (GWAPs) enable human contributors to carry out computation tasks as a side effect of playing online games~\cite{vonAhn2008}. % An example from the area of computational biology is the Phylo game (phylo.cs.mcgill.ca) that disguises the problem of multiple sequence alignment as a puzzle like game thus “intentionally decoupling the scientific problem from the game itself” (Kawrykow et al, 2008). The challenges in using GWAPs in scientific context are in designing appealing games and attracting a critical mass of players.
Finally, altruistic crowdsourcing refers to cases where a task is carried out by a large number of volunteer contributors, such as in the case of the Galaxy Zoo (www.galaxyzoo.org) project where over 250K volunteers willing to help with scientific research classified Hubble Space Telescope galaxy images (150M galaxy classifications). 


Human computation methods have been used to solve a range of knowledge acquisition tasks, as we discuss in Section~\ref{ss:HCKAq}.  Some of the newer approaches, namely ZenCrowd~\cite{Demartini2012}  and CrowdMap~\cite{Sarasua2012}, are making a move towards a tighter integration of HC methods into computational workflows to solve Semantic Web specific tasks such as entity linking or ontology matching. This is an emerging computational paradigm which goes beyond mere data collection and embeds the HC paradigm into adaptive knowledge extraction workflows. We refer to it as \textit{Embedded Human Computation (EHC)}. In the area of Natural Language Processing (NLP), where the use of HC methods is highly popular~\cite{Sabou_Bontcheva_Scharl_2012}, there already exists an effort towards supporting easy integration of HC methods into computation workflows: the GATE Crowdsourcing Plugin is a new component in the popular GATE NLP platform that allows inserting HC tasks into larger NLP workflows, from within GATE's user interface~\cite{Bontcheva2014}. %After concluding that micro-workers are a viable alternative for verifying subclass-superclass relations, 
Noy and colleagues~\cite{Noy2013} introduce a vision for similar tool support that would facilitate the integration of crowdsourcing into ontology engineering workflows. In this paper we investigate the possibility of such a tool, and make the following contributions: 


%RQ1: Which tasks can be crowdsourced? How do they fit in the OE workflow?
%RQ2: How to provide tool support for crowdsourcing in OE?

\begin{enumerate}
\item We distill a set of common HC tasks that are likely to be common to solving a variety of ontology engineering tasks and which should be implemented by the desired tool support (Section ~\ref{ss:HCKAq}).

\item We present a tool, the uComp Prot\'eg\'e plugin, which allows ontology engineers to crowdsource tasks directly from within the popular ontology engineering tool and as part of their ontology engineering workflows, thus enabling novel computational workflows in the spirit of Embedded Human Computation (Section~\ref{s:plugin}).

\item We evaluate some of the functionality of the plugin to estimate the improvements made possible over manually solving a set of tasks in terms of time and cost reductions, while maintaining good data quality (Section~\ref{sec:eval}).

\end{enumerate}

\section{Use of Human Computation for Knowledge Acquisition}\label{ss:HCKAq}

HC methods have been used to support several knowledge acquisition and, more specifically, ontology engineering tasks. To provide an overview of these methods we will group them along the three major stages of the Semantic Life-cycle as identified by Siorpaes in~\cite{Siorpaes2008}.%, where Stage 1 and 2 cover our notion of ontology engineering [TBD - clarify]. 
The main points of the following discussion are summed up in Table \ref{table:tasksFromRW}.

\subsubsection{Stage 1: Build and maintain Semantic Web vocabularies}. Already in 2010, Eckert and colleagues~\cite{Eckert2010} relied on MTurk micro-workers in the process of building a concept hierarchy in the philosophy domain. Judgements collected from micro-workers complemented the output of an automatic hierarchy learning method and focused on two main tasks: judging the relatedness of concept pairs (5-points scale between unrelated and related) and specifying the level of generality between two terms (more/less specific than). 
Noy and colleagues~\cite{Noy2013} focus on the task of verifying subclass-superclass relations that make up the ontology hierarchy as a critical task while building ontologies. 
%Eckert - end goal: construction of hierarchies by aggregating human expert feedback on the relatedness and relative generality of terms; domain philosophy; Similar to us: "automatic OL methods are often weak on the task of determining the type of relation that holds between two terms" - I do not understand all details here ...

Games with a purpose, have also been used to support the process on ontology creation. 
%OntoPronto - decide whether a term is a class or an instance; then they relate this class to the most specific relevant PROTON class, therefore extending PROTON
 OntoPronto game~\cite{Siorpaes2008} aims to support the creation and extension of Semantic Web vocabularies. Players are presented with a Wikipedia page of an entity and they have to (1) judge whether this entity denotes a concept or an instance; and then (2) relate it to the most specific concept of the PROTON ontology, therefore extending PROTON with new classes and instances. Climate Quiz~\cite{Scharl2012a} is a Facebook game where players evaluate whether two concepts presented by the system are related (e.g. “environmental activism”, “activism”), and which label is the most appropriate to describe this relation (e.g. “is a sub-category of”). The possible relation set contains both generic (“is a sub-category of”, “is identical to”, “is the opposite of”) and domain-specific (“opposes”, “supports”, “threatens”, “influences”, “works on/with”) relations. %Two further relations, “other” and “is not related to” were added for cases not covered by the previous eight relations. The game’s interface allows players to switch the position of the two concepts or to skip ambiguous pairs.
 Finally, Guess What?!~\cite{Markotschi2010} goes beyond eliciting or verifying relations between concepts and aims to create complex axioms to describe concepts in an ontology. The game explores instance data available as linked open data. Given a seed concept (e.g., banana), the game engine collects relevant instances from DBpedia, Freebase and OpenCyc and extracts the main features of the concept (e.g., fruit, yellowish) which are then verified through the collective process of game playing. The tasks performed by players are: (1) assigning a class name to a complex class description (e.g., assign $Banana$ to $fruit \& yellow \& grows\ on\ trees$) and (2) verifying previously generated class definitions.



\subsubsection{Stage 2: Align Semantic Web vocabularies}
The CrowdMap system enlists micro-workers to solve the ontology alignment task~\cite{Sarasua2012}. It relies on two types of atomic HITS: the first one asks crowdworkers to verify whether a given relation is correct ("Is conceptA the same As conceptB? yes/no "); the second task, requests micro-workers to specify how two given terms are related, in particular by choosing between sameAs, isAKindOf and notRelated. CrowdMap is designed to allow sameAs, subsumption or generic mappings between classes, properties and axioms, but currently it only supports equivalence and subsumption mappings between classes. SpotTheLink is a GWAP that focuses on aligning Semantic Web vocabularies and has been instantiated to align the eCl@ss and UNSWPC~\cite{Siorpaes2008} as well as the DBpedia and PROTON ontologies~\cite{Thaler2011a}. The final version of the game solves ontology alignment through two atomic tasks: (1) choosing a related concept -- given a DBpedia concept they need to choose and agree upon a related PROTON concept; (2) specifying the type of relation between two concepts in terms of equivalence or subsumption.

%[TBD if time and place] Community focused efforts in Noy2013

\subsubsection{Stage 3: Annotate content and maintain annotations} ZenCrowd~\cite{Demartini2012} focuses on the entity linking problem, where crowd-workers are used to verify the output of automatic entity linking algorithms. Concretely, given a named entity, e.g., "Berlin", and a set of DBpedia URLs generated automatically, crowd-workers have to choose all the URLs that represent that entity or "None of the above" if no URL is suitable. In essence, this is an annotation task. WhoKnows?~\cite{Waitelonis2011} and RISQ!~\cite{Wolf2011} are two games with a purpose which rely on similar mechanisms: they use linked open data (LOD) facts to generate questions and use the answers to (1) evaluate property rankings (which property of an instance is the most important/relevant); (2) detect inconsistencies; (3) find doubtful facts. The obtained property rankings reflect the “wisdom of the crowd” and are an alternative to semantic rankings generated algorithmically based on statistical and linguistic techniques. The games differ in the gaming paradigm they adopt. While WhoKnows?! uses a classroom paradigm and aims towards being an educational game, RISQ! is a Jeopardy-style quiz game. 

%[TBD: More: Celino; Sioarpaes ebay and video annotations]

\begin{table}
%\footnotesize

\center
\begin{tabular}{|l|l|c|c|} \hline
\textbf{SW Life-cycle}&\textbf{Approach}&\textbf{Genre}& \textbf{Solved Task}\\ 
\textbf{Stage}&&&\\ \hline


Stage 1: Build and & InPho~\cite{Eckert2010} & MLab & (T3) Specification of Relation Type (subs)\\ 
\cline{4-4}
	maintain 				   & & & (T1) Specification of Term Relatedness \\ 
					   
 \cline{2-4}
					   
Semantic Web & Noy~\cite{Noy2013} & MLab & (T2) Verification of Relation Correctness (subs) \\ 

 \cline{2-4}
vocabularies & OntoPronto~\cite{Siorpaes2008} & GWAP & Class vs. instance decisions \\ 
 \cline{4-4}
 &&& (T3) Specification of Relation Type (subs/instOf) \\ 
 \cline{2-4}
 
&Climate Quiz~\cite{Scharl2012a} & GWAP & (T3) Specification of Relation Type (8 relations) \\
 \cline{2-4}
 
&Guess What?!~\cite{Markotschi2010}& GWAP & Verify complex class definitions \\ 
 \cline{2-4}
& & & Generate class names for complex defs \\ \hline
\hline
 Satge 2: Align &CrowdMap~\cite{Sarasua2012} & MLab & (T2) Verification of Relation Correctness (subs/eqv) \\
 \cline{4-4}
Semantic Web  & && (T3) Specification of Relation Type (subs/eqv)  \\ 
 \cline{2-4}
Vocabularies &SpotTheLink~\cite{Thaler2011a} & GWAP & (T1) Specification of Term Relatedness \\
 \cline{4-4}
& & & (T3) Specification of Relation Type (subs/eqv) \\ 
 \hline
Stage 3: Annotate & ZenCrowd~\cite{Demartini2012} & MLab & Text to URL mapping (annotation) \\
 \cline{2-4}
content and maintain & WhoKnows?~\cite{Waitelonis2011}& GWAP& Answering quiz questions\\
 \cline{2-4}
Annotations &RISQ!~\cite{Wolf2011}&GWAP& Answering quiz questions\\ \hline

\end{tabular}
\caption{Overview of  crowdsourcing approaches used to address tasks in various stages of the Semantic Web life-cycle~\cite{Siorpaes2008}, their genres and the type of crowdsourcing tasks that they employ.}
\center  \label{table:tasksFromRW}
  \end{table}
  
  
 \subsection{Typical Crowdsourcing Tasks in Ontology Engineering}\label{ss:crowdtasks}

Based on the analysis of the crowdsourcing methods used to support ontology engineering tasks, it emerges that they often converge towards using a range of typical crowdsourcing tasks as follows. 

\begin{description}
\item[T1. Specification of Term Relatedness.]  Crowd-workers need to judge whether two terms (typically representing ontology concepts) are related or not. In some cases they are presented with pairs of terms (InPho~\cite{Eckert2010}) while in others they might need to choose a most related term from a set of given terms (SpotTheLink~\cite{Thaler2011a}). This type of crowdsourcing task is suitable to be used in diverse ontology engineering stages, for example, both in ontology creation scenarios (InPho~\cite{Eckert2010}) and in ontology alignment ones (SpotTheLink~\cite{Thaler2011a}). 

\item[T2. Verification of Relation Correctness.] Presented with a pair of terms (typically representing ontology concepts) and a relation between these terms, crowd-workers are required to judge whether the suggested relation holds or not. The majority of work we reviewed report on verifying generic ontology relations such as equivalence~\cite{Sarasua2012} and subsumption~\cite{Noy2013,Sarasua2012}, which are relevant both in ontology evaluation~\cite{Noy2013} and ontology alignment scenarios~\cite{Sarasua2012}. 

\item[T3. Specification of Relation Type.] In these kinds of tasks, crowd-workers are presented with two terms (typically corresponding to ontology concepts) and can choose from a set of given relations the relation that best relates the terms. Most efforts focus on the specification of generic ontology relations such as equivalence (Climate Quiz~\cite{Scharl2012a}, CrowdMap~\cite{Sarasua2012}, SpotTheLink~\cite{Thaler2011a} ), subsumption(Climate Quiz~\cite{Scharl2012a}, InPho~\cite{Eckert2010}, OntoPronto~\cite{Siorpaes2008}, CrowdMap~\cite{Sarasua2012}, SpotTheLink~\cite{Thaler2011a} ), disjointness (Climate Quiz~\cite{Scharl2012a}) or instanceOf (OntoPronto~\cite{Siorpaes2008}, Climate Quiz~\cite{Scharl2012a}). The verification of domain-specific named relations such as performed by Climate Quiz~\cite{Scharl2012a} is less frequent. 

% [BTW, since this is a complex task it could be split up into a sequence of 3 simpler tasks: 1) given two terms workers agree whether these are related or not; 2) those pairs that were judged related are then passed to another task where a correct relation is selected; 3) in the third task, the quality of the relations is checked - practically T2] Available relation labels are some predefined (like subClassOf) and those from the target ontology (all ObjectProperties) -- if more than 20 .. spread over multiple windows (depending on window size) Have a limit of 5 relation labels in the Prot\'eg\'e interface All pairs with a certain relation (defined by the user in a text field, eg “relation”) are sent to the uComp API (or directly to CF). If you want to use just a subset of relations: have a window where you select (checkboxes) the actual pair to be sent. In CrowdFlower -- also have the choice to add a free text label output: be able to sort by certainty from CF ..

 %This is also a very difficult task in OL in general - the
\item[T4. Verification of Domain Relevance.]  For this task, the crowd-workers confirm whether a given term is relevant for a domain of discourse. This task is mostly needed to support scenarios where ontologies are extracted using automatic methods, for example, through ontology learning. 
\end{description} 


The core crowdsourcing tasks we discuss above have been used by several approaches and across diverse stages of ontology (knowledge) engineering, therefore they are likely to be of interest in a wide range of ontology engineering scenarios. Therefore, they guided the development of our plugin, which currently supports tasks T2 and T4, as well as partially T3.


\section{The uComp Prot\'eg\'e Plugin}~\label{s:plugin}

\begin{figure*}[htb]
\centering
{\centering \resizebox*{1.0\textwidth}{!}{\includegraphics{images/process.png}}}
 \caption{\label{fig:process} Main stages when using the uComp plugin.}
\end{figure*}

In order to support ontology engineers to easily and flexibly integrate crowdsourcing tasks within their ontology engineering workflows, we implemented a crowdsourcing plugin in Prot\'eg\'e, one of the most widely used ontology editors. The typical workflow of using the plugin involves the following main stages (as also depicted in Figure~\ref{fig:process}).

\begin{description}
\item[1. Task Specification.] An ontology engineer using Prot\'eg\'e can invoke the functionalities of the plugin from within the ontology editor at any time within his current work. The plugin allows specifying some well defined ontology engineering tasks, such as those discussed in Section~\ref{ss:crowdtasks} above. The view of the plugin that is appropriate for the task at hand is added to the editor's user interface via the \emph{Window $\rightarrow{}$ Views} menu. The ontology engineer then specifies the part of the ontology to verify (eg. a specific class or all classes in the ontology), provides additional information and options in the plugin view and then starts the evaluation. Crowdsourced tasks can be canceled (or paused) anytime during the crowdsourcing process. We further detail the plugin's functionality in Section~\ref{ss:functionality}.

\item[2. Task Request]  The plugin uses the uComp API\footnote{TBD: is there an URL for this} to request the processing of the task by the crowd.


\item[3. Creation of Crowdsourcing Tasks.] The crowdsourcing process happens through the uComp platform\footnote{The platform is currently being developed as part of the uComp project, \url{http://www.ucomp.eu/}}, a hybrid-genre crowdsourcing platform which facilitates various knowledge acquisition tasks by flexibly crowdsourcing the received tasks to games with a purpose and mechanised labour platforms alike (in particular, CrowdFlower)~\cite{Sabou2013}. Depending on user settings, the uComp API delegates the job to a GWAP, to CrowdFlower or to a combination of these two genres. In Section~\ref{ss:crowdtasks} we present the crowdsourcing tasks created by the uComp platform.

\item[4\&5 Collection of Crowd Results.] The uComp platform collects and combines crowd-work harvested through various genres and sends the data to the plugin.

\item[6. Result Presentation and Interpretation.] As soon as available, the plugin presents the results to the ontology engineer and saves them in the ontology. All data collected by the plugin %which should be persistent [TBD - what is persistent?]
is stored in the ontology in \texttt{rdfs:comment} fields, for example information about the ontology domain, the crowdsourcing job ID, and the crowd-created results. Depending on the result, the ontology engineer will perform further actions such as deleting parts of the ontology which have been validated as non-relevant.
\end{description}

\subsection{Plugin Functionality}\label{ss:functionality}

The plugin provides a set of views for crowdsourcing the following tasks:

\begin{itemize}
\item Verification of Domain Relevance (T4)
\item Verification of Relation Correctness - Subsumption (T2)

\item Verification of Relation Correctness - InstanceOf (T2) - the verification of \emph{instanceOf} relations between an individual and a class, i.e.~the crowd helps to verify if a given \emph{instanceOf} is valid.

\item Specification of Relation Type (T3) is a Prot\'eg\'e view component that collects suggestions for labeling unlabeled relations by assigning to them a relation type from a set of relation types specified by the ontology engineer.

\item Verification of Domain and Range where crowd workers validate whether a property's \emph{domain} and \emph{range} restrictions are correct.
This results in two separate sub-tasks (domain, range).
 [<=TBD: what does this do exactly] 

\end{itemize}


In this paper we focus on the description and evaluation of the first two functionalities.

%Florian: The CF key has to be associated with the uComp-API key, therefore it has to be communicated to the uComp-API team (see http://soc.ecoresearch.net/facebook/election2008/ucomp-quiz-beta/api/v1/documentation/)
%Florian: The uComp-API key itself must be put into a textfile named "ucomp_api_key.txt" at the users home directory, in the folder ".Protege" (which is created by Prot\'eg\'e during installation on both Windows and Linux plattforms) 

%Florian: all information about the task is stored (depends on the kind of task): domain, validation of whole subtree going on?, additional information, sent to crowdflower or ucomp-quiz, ucomp-api job-id, ...

% give an example SCREEENSHOT with a quick introduction 
\begin{figure*}[htb]
\centering
{\centering \resizebox*{1.0\textwidth}{!}{\includegraphics{images/c_rel_check_start.png}}}
 \caption{\label{fig:screen_cr} The interface of the uComp Class Validation view used to create a Verification of Domain Relevance (T4) task.} %TBD: Gerhard: could you make a new screenshot, with another term that is more know and also shown highlighte in the left side hierarchy list - "climate" would be great. Also, can you add some example text in "Additional information for validators": for example: "You can check any external resources if needed."}
\end{figure*}
%[TBD: Gerhard: could you make a new screenshot, with another term that is more know and also shown highlighte in the left side hierarchy list - "climate" would be great. Also, can you add some example text in "Additional information for validators": for example: "You can check any external resources if needed."]



% COMMON FIELDS in the UI: domain and additional information & validate subtree
% TASKs
\textbf{Verification of Domain Relevance (T4)} is supported by the ``uComp Class Validation'' view of the plugin and crowdsources the decision of whether a concept (class) is relevant for a domain. First, the ontology engineer adds the corresponding view (\emph{Window} $\rightarrow{}$ \emph{Views} $\rightarrow{}$ \emph{Class Views} $\rightarrow{}$ \emph{uComp Class Validation}) to the editor's UI. Figure~\ref{fig:screen_cr} shows the screenshot of this view for the class ``climate'' before initiating the verification. [TBD - how does the returned result look like? Do we have a screenshot?] The plugin view's interface contains the following information:

\begin{description}
\item[Task Specific Information] such as the concept selected by the user for validation. This part of the view is diverse among different functionalities.
\item[Generic information] such as the \emph{domain} of the ontology, i.e., the field of knowledge which the ontology covers, is present in all views of the plugin. If entered once, the domain will be stored in the ontology (as \texttt{rdfs:comment}) and be pre-filled subsequently, but it can also be changed at any time.
\item[Additional information] For every task, the plugin contains a predefined task description (typically including examples) which is presented to the crowd-worker. If the ontology engineer wants to extend this task description, (s)he can provide more guidelines in the \emph{additional information} field. This functionality is present in all the views of the plugin.
\item[Recursive control] In many cases the ontology engineer wants to perform a task (e.g., domain relevance validation) not only for the current class, but for a larger part of or even the whole ontology. If the \emph{Validate subtree} option is selected, the plugin crowdsources the specified task for the current concept and all its subconcepts recursively. To apply the functionality to the entire ontology, the plugin is invoked from the uppermost class, i.e., (\emph{Thing}).
\item[\texttt{GO} button] to start the crowdsourcing process. 
\end{description}


%Florian: domain will be stored as rdfs:comment in the head of the ontology

% T1. Verification of Domain Relevance. Is a concept/instance relevant for a domain?
% T2. Verification of Relation Correctness. Does a certain relation between two ontology entities hold? These could be a set of generic relations (sameAs, subClassOf, instanceOf), but also arbitrary named relations to be specified by the ontology engineer. The crowd here would have to vote (yes/no) for a given triple (Subject - Relation - Object). This task is the focus of [1].

\begin{figure*}[htb]
\centering
{\centering \resizebox*{1.00\textwidth}{!}{\includegraphics{images/sc_subclass_val.png}}}
 \caption{\label{fig:screen_sub}Screenshot showing the interface for subClassOf relation validation, including the display of results}.
\end{figure*}


\textbf{Verification of Relation Correctness - Subsumption (T2).}
The second task is the verification of relation correctness, more precisely the verfication of IS-A (subClassOf) relations between classes.
The corresponding view is named \emph{Class Views} $\rightarrow{}$ \emph{uComp SubClass Validation}. When selecting a class in Prot\'eg\'e,
the plugin automatically detects its superclasses (if any) and fills the boxes in the plugin UI.
As soon as results are available these are presented in the UI, as shown in Figure~\ref{fig:screen_sub}. The screenshot gives an example
with one evaluator, who rated the \emph{IS-A} relation between ``education'' and ``business'' as invalid. As the majority of ratings is negative,
a button to remove the relation is displayed.

\subsection{Crowdsourcing Task Interfaces}\label{ss:crowdtasks}

Upon receiving the request from the Prot\'eg\'e plugin, the uComp API selects the appropriate crowdsourcing genres and creates the relevant crowdsourcing jobs. Currently the platform can crowdsource tasks either to games with a purpose such as Climate Quiz or to the CrowdFlower mechanized labour platform. A hybrid-genre crowdsourcing strategy is currently being developed. In this paper, we test the plugin by crowdsourcing only to the CorwdFlower platform. 

Figure~\ref{fig:CFUI} depicts the crowdsourcing interfaces created automatically by the uComp platform for the two tasks discussed above, namely the verification of domain relevance and the validation of subsumption relations. The uComp platform requires only the task data from the Prot\'eg\'e plugin and it provides relevant instructions as well as gold units to all tasks. Additionally, each crowdsourcing interface is extended with straightforward verification questions (i.e., typing some letters of the input terms) which force crowd-workers to actually read the data of each crowdsourcing unit and therefore take an informed decision rather than just clicking arbitrarily one of the responses.


\begin{figure}%[htbp]
   \centering
   \begin{tabular}{c c}
   \includegraphics[scale=0.27]{images/CFdomainRel.png}&
   \includegraphics[scale=0.27]{images/CFsubclasscheck.png}\\
   (a) & (b)
   \end{tabular}
   \caption{Generated CrowdFlower job interface for (a) the Verification of Domain Relevance (T4) and (b) the Verification of Relation Correctness (T2) tasks.}
   \label{fig:CFUI}
\end{figure}


To ensure a good quality output, by default all created jobs are crowdsourced to Level 3 CrowdFlower contributors which are the contributors delivering, on average, the highest quality work. Also, for the moment we assume that the verified ontologies will be in English and therefore we restrict contributors to the main English speaking countries: Australia, United Kingdom and United Sates. In each created job we present 5 units per page and for each unit we collect 5 individual judgements. A price per task of \$0.05 was specified for all jobs. Future versions of the plugin will provide higher control over task settings from within Prot\'eg\'e.

\subsection{Implementation and Installation Details} 
 Prot\'eg\'e was developed in Java, it can easily be extended in the form of \emph{plugins} which are typically Java Archive (.jar) files
stored in the Prot\'eg\'e \texttt{plugin} directory. The most common form of a Prot\'eg\'e plugin is a \emph{view plugin}, which implements a single view for a specific area of an ontology (e.g. classes, individuals, object properties).
%Florian: Prot\'eg\'e completely was programmed in Java, therefore all plugins also are programmed in Java. Since Prot\'eg\'e was developed very modular, it is quite easy to create simply plugins and integrate them into Prot\'eg\'e. 
%Florian: All Prot\'eg\'e plugins are so called jar-Files (Java Archive), and contains the compiled source code and all needed libraries. The most common form of a Prot\'eg\'e plugin is a view plugin, which implements a single view for a specific area of an ontology (e.g. classes, individuals, object properties, ...)

% installation / SETUP 
\textbf{Installation and setup.}
The plugin has been included into Prot\'eg\'e's central registry and can be installed from within Prot\'eg\'e with
the \emph{File $\rightarrow{}$ Check for Updates} menu item. A window titled \emph{Automatic Update} will pop up, where
you can selected the \emph{uComp Crowdsourcing Validation plugin} from the list of downloads. The download files contain
the plugin itself, and a detailed documentation in PDF format.
To use the plugin you need to your uComp-API key\footnote{Request a key from the uComp team, see \url{http://soc.ecoresearch.net/facebook/election2008/ucomp-quiz-beta/api/v1/documentation/}} in a file named \texttt{ucomp\_api\_settings.txt} in folder \texttt{.Protege}. Details are found in the plugin documentation.

\section{Evaluation}
\label{sec:eval}

%\subsection{Evaluation Scenario - Ontology Engineering using Ontology Learning}

%Eckert - end goal: construction of hierarchies by aggregating human expert feedback on the relatedness and relative generality of terms; domain philosophy; Similar to us: "automatic OL methods are often weak on the task of determining the type of relation that holds between two terms" - I do not understand all details here ... 


% <added by gerhard: 2014-05-07 17:00>
Ontology construction from scratch is a time-consuming and complex process and it is often bootstrapped by re-using existing ontologies or ontologies derived by automatic methods from non-ontological resources (e.g., text corpora, folksonomies, databases etc). Ontology learning methods, for example, automatically extract ontologies from a variety of unstructured and structured resources or a combination thereof. Although the performance of ontology learning methods is improving [Cimiano+Buitelaar], the extracted ontologies typically contain questionable or wrong ontological elements and require a phase of verification and redesign (especially pruning) by the ontology engineer. The ontology verification phase typically involves, among others, checking that the ontology concepts are relevant to the domain of interest and that the extracted subsumption relations are correct. Since the uComp plugin supports both these tasks, we will evaluate these tasks in the scenario of ontology engineering when reusing ontologies obtained through ontology learning.

%In the ontology engineering scenario in this paper we aim to simplify the phase of pruning an ontology of concepts (classes) not relevant to the domain, as well as of \emph{isA} relations that are not valid.

%The uComp Prot\'eg\'e plugin also supports other tasks, such as validating \emph{instanceOf} relations or checking \emph{domain/range} restrictions (see below), but these are not part of the evaluation in this publication.

The ontology learning system used to generate the input ontologies~\cite{wohlgenannt2012} is geared towards learning lightweight domain ontologies from heterogeneous sources (text, social media, structured data). %As we are learning domain ontologies in periodic intervals (monthly) from scratch, there is a focus on ontology evolution experiments, too.
 The ontology learning process starts from a small seed ontology 
(typically just a few concepts and relations), and extends it with additional concepts and relations. 
based on evidence collected from heterogeneous evidence sources 
with methods such as co-occurrence analysis or Hearst patterns.
The neural network technique of spreading activation is the main algorithm used to determine the most important new concepts
from the plethora of evidence. After positioning the new concepts in the ontology, the extended ontology serves as
new seed ontology, and another round of extension is initiated. The system currently stops after three extension iterations.
% </added by gerhard: 2014-05-07 17:00>


\subsection{Evaluation Goal}
The goal of the evaluation is to assess the improvements that the uComp Plugin could enable in a ontology engineering scenario using ontology learning in terms of typical project completion aspects such as time, cost and quality of output. The usability of the plugin is an additional criteria that should be evaluated. Concretely, our evaluation goals can be summarised into the following questions:

\begin{description}
\item[Time] \textit{How does the use of the plugin affect the time needed to perform ontology engineering tasks?} - We distinguish here the total task time ($T_{tt}$) as the time taken from the start of the ontology engineering task until its finalisation; and the time of the ontology engineer spent actively in the task ($T_{oe}$). In a crowdsourced scenario, $T_{oe} < T_{tt}$, because the ontology engineer is only actively working during the outsourcing of the task and the review of the result [??]. In contrast, in a traditional scenario $T_{oe} = T_{tt}$. What is of interest to us is the time reduction ratio.%, that is $\frac{T_{tt}-T_{oe}}{T_{tt}}$. (this will be computed as an average over multiple measurements, and over various ontology engineering tasks).

\item[Cost] \textit{Are there cost benefits associated with the use of the plugin?}  We compute costs related to payments for the involved work-force, that is payments to ontology experts and payments to crowd-workers. Costs of ontology experts are computed by multiplying the time they spend on the task ($T_{oe}$) with an average salary. In order to allow comparison to other similar cost-focused studies~\cite{Poesio2012}, the wage of a research scientist was assumed to be \$54,000 per annum.

\item[Quality] \textit{What are the implications on the quality of the resulting output when using the Plugin?} Several earlier studies have previously shown that the quality of various knowledge acquisition tasks performed by crowd-workers is, in general, similar to (or even better than) the quality of tasks performed by ontology engineers~\cite{Thaler2012,Noy2013,Sabou2013a} . While the quality of the obtained data is not the core focus of our evaluation, we expect to obtain results similar to those already published.

\item[Usability] \textit{Is the plugin usable?} As any end-user tool, the plugin should be easy to understand and use by the average ontology engineer already familiar with the Prot\'eg\'e environment.
\end{description}

\subsection{Evaluation Setup}

The setup involves a group of 8 ontology engineers which perform the same tasks over the same datasets but using two different approaches. In the first setting (Setting 1), ontology engineers used the traditional (that is manual) approach to perform the ontology engineering tasks. In Setting 2, the ontology engineers used the Plugin to crowdsource (that is, create and launch) the same ontology engineering tasks. They were given a short tutorial about the plugin (30 minutes) and filled in a short usability questionnaire about using the plugin. The performance of the group in the two diverse settings is then compared along time, cost and quality dimensions.

\subsubsection{Evaluation Data}
The input to all evaluation tasks are ontologies generated by the ontology learning algorithm described above and in~\cite{wohlgenannt2012} (primarily) from textual sources. %MS: I removed this text not because it is bad, but because it is not very relevant for the experiments and it might open us to some critique. Gerhard - if you really want it in, please include it again. For every ontology snapshot, ie. every result of an ontology learning stage, as well as the resulting ontology the ontology learning system exports an OWL file (using the Turtle seralization format, which can be converted to RDF/XML easily). The conversion of our internal format for lightweight ontologies is straightforward for concepts (which resemble to OWL classes). WordNet hyper- and hyponym relations are mapped to the OWL subClassOf property. The only challenging aspect is the representation of unlabeled relations in OWL. Our system creates ObjectProperties named "relation\_n", where "n" is an auto-incrementing number, as it is not possible to use the same ObjectProperty more than once. The label for those relations is plainly "relation". The periodically created versions of the domain ontology can be uploaded to a triple store and are thereby accessible via the SPARQL.
We evaluate the plugin over two ontologies covering two diverse domains (climate change and finance). Table~\ref{table:ontology_data} lists some statistics about the sizes of these ontologies.

\begin{table}
%\footnotesize

\center
\begin{tabular}{|l|c|c|} \hline
\textbf{Nr.} &\textbf{Climate Change}&\textbf{Finance}\\ 
&\textbf{Ontology}&\textbf{Ontology}\\\hline


\textbf{Classes} & 101  & 77 \\ \hline
\textbf{Relations} & 61  & 50  \\ \hline
\textbf{IsA Relations} & 43  & 20  \\ \hline
\textbf{Unnamed Relations} & 18  & 30 \\ \hline

\end{tabular}
\caption{Overview of the ontologies used in the evaluation.}
\label{table:ontology_data}
\end{table}


\subsection{Evaluation Tasks}
We perform the evaluation of the plugin over two different ontology engineering tasks in order to 1) test different functionalities of the plugin; 2) obtain evaluation results over a range of tasks. These tasks are:

\begin{description}
\item[Task 1: Check concept relevance for a domain] For each concept of the ontology decide whether it is relevant for the domain in question (in our case, climate change and finance). Input: concept (class) and domain name; Output: true/false ratings -- we use a uComp\_class\_relevance annotation in Prot\'eg\'e to save the results -- domain experts use true/false 

\item[Task 2: Check the correctness of isA relations] For all subsumption relations in the ontology verify whether they are correct. Manual experts set the value for a certain annotation uComp\_subclassof\_check.
\end{description}




\subsection{Evaluation Results}


% new wohlg -- time
% times:
%     Marta, FJE, Gerhard, Matyas, Michael, Olga, Philipp, Stefan
% CC: 
% T1: [29,    32  ,    35    , 31   , 22    , 26   , 25    , 19]
% T2: [20   , 16  ,    20    , 23   , 30    , 16   , 35    , 24]
% 
% Euro: 
% T1: [15   , 9   ,    30    ,        30    , 19   , 23, 23]
% T2: [09   , 10  ,    24    ,        12    , 11   , 17, 22]

%python time\_analysis.py
%27.375
%avg vari 25.234375
%stddev 5.02338282435

%23.0
%avg vari 38.75
%stddev 6.22494979899

%21.2857142857
%avg vari 50.4897959184
%stddev 7.10561720883

%15.0
%avg vari 31.4285714286
%stddev 5.60611910581



%OLD
%\begin{table}
%%\footnotesize
%\center
%\begin{tabular}{|c|c|c|c|c|} \hline
 %   & \multicolumn{2}{|c|}{\textbf{Climate Change}} & \multicolumn{2}{c|}{\textbf{Finance Ontology}}  \\
  %      & \multicolumn{2}{|c|}{\textbf{Ontology}} &   \\  \hline
%                                     & Task 1       & Task 2     & Task 1     & Task 2    \\ \hline
%\textbf{Manual validation - AVG time}&  27.375 (8)  & 23.0 (8)   & 21.28 (7)   & 15.0 ( 7) \\ \hline
%\textbf{Manual validation - STDDEV}  &  5.023 (8)   & 6.225 (8)  & 7.106 (7)  & 5.610  (7) \\ \hline
%\textbf{CF     validation - time}  &     &  &   &  \\ \hline
%\textbf{Time reduction / ratio}  &     &  &   &  \\ \hline
%\end{tabular}
%\caption{Time measures .. TODO.}
%\label{table:eval_qual}
%\end{table}


\textbf{Task Duration}. Table~\ref{table:eval_time} lists the task durations for the two ontologies and the two settings, detailed in terms of the average time intervals spent by the ontology engineer ($T_{oe}$), by using human computation ($T_{hc}$) and the total time of the task ($T_{tt}=T_{oe} + T_{hc}$). In the case of the second setting, the time needed for the ontology engineers to crate and launch the crowdsourcing task was on average between 1 and 2 minutes. To simplify calculations, we chose to take the average time as 2 minutes across all tasks. We notice that the time reduction ratio for the ontology engineer across the two settings (computed as the ontology engineering time in Setting 1 divided by the same time in Setting 2) is significant and ranges from a 13.7 fold reduction to a 7.5 fold reduction, on average.

\begin{table}
\center
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
    & \multicolumn{6}{|c|}{\textbf{Climate Change}} & \multicolumn{6}{c|}{\textbf{Finance}}  \\
            & \multicolumn{6}{|c|}{\textbf{Ontology}} & \multicolumn{6}{|c|}{\textbf{Ontology}}   \\ 
  \cline{2-13}
             &  \multicolumn{3}{|c|}{\textbf{Task 1}}      & \multicolumn{3}{|c|}{\textbf{Task 2}}     & \multicolumn{3}{|c|}{\textbf{Task 1}}    & \multicolumn{3}{|c|}{\textbf{Task 2}}   \\ 
    \cline{2-13}
  &$T_{oe}$& $T_{hc}$   & $T_{tt}$      &$T_{oe}$& $T_{hc}$   & $T_{tt}$   &$T_{oe}$& $T_{hc}$   & $T_{tt}$   &$T_{oe}$& $T_{hc}$   & $T_{tt}$       \\ \hline
 \textbf{Setting 1 (Avg)} &27.4 & 0   & 27.4    &23.0 & 0   & 23.0  & 21.3 & 0   & 21.3   &15.0 & 0   & 15.0  \\ \hline
 \textbf{Setting 1 (StdDev)} &5 & 0   & 5    &6.2 & 0   & 6.2  & 7.1 & 0   & 7.1   &5.6 & 0   & 5.6  \\ \hline
  \textbf{Setting 2 (Max)} & 2 & ?   & ?    & 2 & ?   & ?  & 2 & ?  & ?   & 2 & ?   & ?  \\ \hline
    \textbf{Time red. ratio} & 13.7 & ?   & ?    & 12.5 & ?   & ?  & 10.65 & ?  & ?   & 7.5 & ?   & ?  \\ \hline
\end{tabular}
\caption{Task durations in minutes  per ontology and setting.}
\label{table:eval_time}
\end{table}


% Times 
%440477, FT1 - running = 2hours; 
%440476, FT1 - running = 2hours; 
%440475, FT1 - running = 3hours; 
%440474, CCT1 - running = 3hours; 
%440473, CCT1 - running = 3hours; 
%440473, CCT1 - running = NYF; 
%440471, FT2 - running = 3hours; 
%440470, FT2 - running = 3hours; 
%440469, FT2 - runing = 4hours; 
%440468, CCT2 - runing = 4hours; 
%440467, CCT2 - runing = 4hours; 
%440464, CCT2 - runing = 6hours; 
\textbf{Costs}. For the cost analysis, we compute average costs for the total task ($C_{tt}$) as the sum of the average cost of the ontology engineer ($C_{oe}$) and the average cost of the crowd-sourced tasks ($C_{hc}$) as detailed in Table~\ref{table:eval_cost}. Considering an annual salary of \$54,000 and a corresponding \$26 hourly wage, average ontology engineering costs were computed based on the average times shown in Table~\ref{table:eval_time}. Cost savings were then computed for each cost category. On average, cost savings for ontology engineer costs are high and range from 92.4\% to 86.15\%, averaged at 89.9\%. For the task as a whole, cost savings are moderate and range from 19.7\% to 60.5\% and average over all tasks to  39\%, thus meaning that in Setting 2 the costs of the total tasks are about 60\% of the tasks solved manually. It is important to note, however, that the task level cost savings will ultimately depend on the cost that ontology engineers are willing to pay to crowd-workers. In our settings, we payed \$0.05 per task and requested 5 judgements for each unit, but a high number of micro-tasks are payed much lower, mostly at \$0.01 per task and request a lower number of judgments, typically 3. [TBD - compute \$0.01, 3 judgments scenario] From the plugin development's perspective, the major goal is reducing ontology engineering costs, as crowdsourcing costs will depend on the time, financial and quality constraints of the ontology engineer and are therefore hard to generalise.


\begin{table}. 
\center
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
    & \multicolumn{6}{|c|}{\textbf{Climate Change}} & \multicolumn{6}{c|}{\textbf{Finance}}  \\
            & \multicolumn{6}{|c|}{\textbf{Ontology}} & \multicolumn{6}{|c|}{\textbf{Ontology}}   \\ 
  \cline{2-13}
             &  \multicolumn{3}{|c|}{\textbf{Task 1}}      & \multicolumn{3}{|c|}{\textbf{Task 2}}     & \multicolumn{3}{|c|}{\textbf{Task 1}}    & \multicolumn{3}{|c|}{\textbf{Task 2}}   \\ 
    \cline{2-13}
  &$C_{oe}$& $C_{hc}$   & $C_{tt}$      &$C_{oe}$& $C_{hc}$   & $C_{tt}$   &$C_{oe}$& $C_{hc}$   & $C_{tt}$   &$C_{oe}$& $C_{hc}$   & $C_{tt}$       \\ \hline
 \textbf{Setting 1 (Avg)} &11.9 & 0   & 11.9   &9.9& 0   & 9.9  & 9.2 & 0   & 9.2   & 6.5 & 0   & 6.5  \\ \hline
 %\textbf{Setting 1 (StdDev)} &5 & 0   & 5    &6.2 & 0   & 6.2  & 7.1 & 0   & 7.1   &5.6 & 0   & 5.6  \\ \hline
\textbf{Setting 2 (Avg)} & 0.9 & 8.48   & 9.38    & 0.9 & 3.58   & 4.48  & 0.9 & 6.49  & 7.39  & 0.9 & 1.67   & 2.57  \\ \hline
 %  \textbf{Cost red. ratio} & 13.2 & -   & 1.2   & 11 & -  & 2.2  & 10.2 & -  & 1.2  & 7.2 & -  & 2.5  \\ \hline
  %    \textbf{Cost red. prec.} & 7.5\% & -   & 78.8\%   & 9.1\% & -  & 45.5\%  & 9.8\% & -  & 80.3\%  & 13.8\% & -  & 39.5\%  \\ \hline
            \textbf{Cost Savings} & 92.4\% & -   & 21.2\%   & 90.1\% & -  & 54.7\%  & 90.2\% & -  & 19.7\%  & 86.15\% & -  & 60.5\%  \\ \hline
\end{tabular}
\caption{Average costs (in \$) for the ontology engineer ($C_{oe}$), crowd-workers ($C_{hc}$) and the entire task ($C_{tt}$) across ontologies and settings.}
\label{table:eval_cost}
\end{table}



%%%%% QUALITY

\textbf{Data Quality.} Lower completion times and costs should not have a negative effect on the quality of the crowdsourced data. Since we do not possess a baseline for either of the two tasks, we will perform a comparative evaluation and contrast inter-rater agreement levels between ontology engineers with those of crowdworkers. We have measured inter-rater agreement using the statistical measure of Fleiss' Kappa used to assess reliability of agreement with a fixed number of raters and categorical ratings assigned to a number of items.

Table~\ref{table:eval_qual} presents inter-rater agreement per task and per setting. 
For the manual setting (Setting 1), inter-rater agreement rates measured with Fleiss' Kappa are consistent and rather high, 
except for the class relevance verification task for the \emph{Climate Change} ontology. 
For Setting 2: Setting 2 is the inter-rater agreement of CF worker groups (CF only)
For Setting 3: Setting 3 is the inter-rater agreement of manual + CF. 


The number of raters per task is given in parantheses. TODO refine
\emph{Percentage valid} reflects the ratio of unit where the majority of raters confirm the validity [MS: I do not get this]. When merging the data from the two groups and combining Fleiss Kappa over the total data, we observe that agreement increases which is a good sign [TBD - refine].

\begin{table}
%\footnotesize
\center
\begin{tabular}{|c|c|c|c|c|} \hline
    & \multicolumn{2}{|c|}{\textbf{Climate Change}} & \multicolumn{2}{c|}{\textbf{Finance}}  \\ 
    \cline{2-5}
        & \multicolumn{2}{|c|}{\textbf{Ontology}} & \multicolumn{2}{c|}{\textbf{Ontology}}  \\ 
        
          \cline{2-5}
                                    & \textbf{Task 1}      & \textbf{Task 2}     & \textbf{Task 1}     & \textbf{Task 2}    \\ \hline
\textbf{Setting 1 - Percentage valid} & 0.71        & 0.5        & 0.72       & 0.15      \\ \hline
\textbf{Setting 1 - Kappa}            & 0.338 (8)   & 0.502 (8)  & 0.500 (7)  & 0.388 (7) \\ \hline
\textbf{Setting 2 CF only - Kappa}    &             &            &            &           \\ \hline
\textbf{Setting 3 Manual + CF (Kappa)}&       (12)  & 0. (12)  & 0.688 (11)  & 0. (11) \\ \hline

\textbf{TODO}                       &             &            &            &           \\ \hline
\end{tabular}
\caption{Inter-rater agreement in each setting and in the combined setting.}
\label{table:eval_qual}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% RESULTS FROM FLEISS KAPPA

% CC Class relevance
% Kappa:
% 8 raters.
% 100 subjects.
% 2 categories.
% p = [0.62375, 0.37625]
% P = [1.0, 0.4642857142857143, 1.0, 0.5714285714285714, 1.0, 0.4642857142857143, 1.0, 0.5714285714285714, 0.75, 1.0, 0.42857142857142855, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.5714285714285714, 0.42857142857142855, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.75, 1.0, 0.75, 1.0, 0.4642857142857143, 0.5714285714285714, 0.75, 0.5714285714285714, 0.5714285714285714, 1.0, 1.0, 0.4642857142857143, 1.0, 0.5714285714285714, 1.0, 0.5714285714285714, 0.5714285714285714, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.4642857142857143, 0.4642857142857143, 0.4642857142857143, 0.75, 0.5714285714285714, 0.42857142857142855, 0.4642857142857143, 0.75, 0.4642857142857143, 0.75, 1.0, 0.42857142857142855, 0.75, 1.0, 0.5714285714285714, 1.0, 0.42857142857142855, 0.5714285714285714, 0.4642857142857143, 0.5714285714285714, 0.42857142857142855, 0.4642857142857143, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.4642857142857143, 0.5714285714285714, 1.0, 1.0, 1.0, 0.75, 0.75, 0.4642857142857143, 0.75, 0.5714285714285714, 1.0, 0.4642857142857143, 0.42857142857142855, 0.5714285714285714, 0.5714285714285714, 0.75, 0.5714285714285714, 0.75, 0.5714285714285714, 0.75, 0.4642857142857143, 0.42857142857142855, 0.5714285714285714, 0.4642857142857143, 0.5714285714285714]
% Pbar = 0.689642857143
% PbarE = 0.530628125
% kappa = 0.338781977814
% 


% CC subclass
% 8 raters.
% 42 subjects.
% 2 categories.
% p = [0.46130952380952384, 0.5386904761904762]
% P = [0.4642857142857143, 1.0, 1.0, 0.75, 1.0, 1.0, 0.4642857142857143, 1.0, 0.5714285714285714, 1.0, 1.0, 0.5714285714285714, 1.0, 0.5714285714285714, 1.0, 0.42857142857142855, 0.4642857142857143, 0.75, 0.75, 0.75, 1.0, 0.4642857142857143, 0.5714285714285714, 0.75, 0.75, 0.42857142857142855, 1.0, 1.0, 0.75, 0.5714285714285714, 0.75, 1.0, 1.0, 0.5714285714285714, 1.0, 0.4642857142857143, 0.4642857142857143, 0.5714285714285714, 0.75, 0.4642857142857143, 0.75, 1.0]
% Pbar = 0.752551020408
% PbarE = 0.502993905896
% kappa = 0.502120834076
% 

% Kappa (with cf_results)
% 9 raters.
% 42 subjects.
% 2 categories.
% p = [0.46825396825396826, 0.5317460317460317]
% P = [0.4444444444444444, 1.0, 1.0, 0.7777777777777778, 1.0, 1.0, 0.5, 1.0, 0.6111111111111112, 1.0, 1.0, 0.6111111111111112, 1.0, 0.6111111111111112, 1.0, 0.4444444444444444, 0.4444444444444444, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 1.0, 0.4444444444444444, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.4444444444444444, 1.0, 1.0, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 1.0, 1.0, 0.6111111111111112, 1.0, 0.5, 0.5, 0.6111111111111112, 0.7777777777777778, 0.5, 0.7777777777777778, 1.0]
% Pbar = 0.76455026455
% PbarE = 0.502015621063
% kappa = 0.527194535796
% percentage true 0.47619047619
% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Euro Class relevance
% Kappa:
% 7 raters.
% 77 subjects.
% 2 categories.
% p = [0.6753246753246753, 0.3246753246753247]
% P = [1.0, 0.7142857142857143, 1.0, 1.0, 0.5238095238095238, 1.0, 0.7142857142857143, 0.7142857142857143, 1.0, 0.7142857142857143, 0.5238095238095238, 0.7142857142857143, 0.7142857142857143, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.5238095238095238, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 0.7142857142857143, 0.7142857142857143, 0.5238095238095238, 1.0, 1.0, 1.0, 1.0, 0.5238095238095238, 0.7142857142857143, 0.42857142857142855, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.5238095238095238, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.5238095238095238, 0.42857142857142855, 1.0, 0.42857142857142855, 1.0, 0.7142857142857143, 0.5238095238095238, 0.42857142857142855, 1.0, 0.5238095238095238, 1.0, 0.5238095238095238, 0.7142857142857143, 0.5238095238095238, 0.42857142857142855, 0.7142857142857143, 0.5238095238095238, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238]
% Pbar = 0.78107606679
% PbarE = 0.561477483555
% kappa = 0.500769230769


%Kappa (with CF results)
%8 raters.
%77 subjects.
%2 categories.
%p = [0.6915584415584416, 0.30844155844155846]
%P = [1.0, 0.75, 1.0, 1.0, 0.5714285714285714, 1.0, 0.75, 0.75, 1.0, 0.75, 0.4642857142857143, 0.75, 0.75, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.5714285714285714, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 0.5714285714285714, 1.0, 1.0, 1.0, 1.0, 0.4642857142857143, 0.75, 0.4642857142857143, 0.75, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 0.5714285714285714, 0.4642857142857143, 1.0, 1.0, 1.0, 1.0, 0.4642857142857143, 0.4642857142857143, 1.0, 0.4642857142857143, 1.0, 0.75, 0.4642857142857143, 0.4642857142857143, 1.0, 0.5714285714285714, 1.0, 0.4642857142857143, 0.75, 0.4642857142857143, 0.4642857142857143, 0.75, 0.5714285714285714, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4642857142857143, 0.5714285714285714, 0.5714285714285714]
%Pbar = 0.788497217069
%PbarE = 0.573389273065
%kappa = 0.504225352113
%percentage true 0.753246753247
%


%Kappa:
%11 raters.
%77 subjects.
%2 categories.
%p = [0.6599763872491146, 0.3400236127508855]
%P = [1.0, 0.8181818181818182, 1.0, 1.0, 0.4909090909090909, 0.8181818181818182, 0.8181818181818182, 0.5636363636363636, 1.0, 0.8181818181818182, 0.45454545454545453, 0.6727272727272727, 0.5636363636363636, 0.45454545454545453, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6727272727272727, 0.4909090909090909, 0.6727272727272727, 1.0, 1.0, 1.0, 0.8181818181818182, 1.0, 0.8181818181818182, 0.8181818181818182, 0.4909090909090909, 1.0, 1.0, 1.0, 0.6727272727272727, 0.5636363636363636, 0.8181818181818182, 0.45454545454545453, 0.8181818181818182, 0.6727272727272727, 0.8181818181818182, 1.0, 1.0, 1.0, 0.8181818181818182, 0.8181818181818182, 0.4909090909090909, 0.45454545454545453, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.45454545454545453, 1.0, 0.5636363636363636, 1.0, 0.6727272727272727, 0.45454545454545453, 0.4909090909090909, 1.0, 0.5636363636363636, 1.0, 0.45454545454545453, 0.8181818181818182, 0.5636363636363636, 0.5636363636363636, 0.6727272727272727, 0.6727272727272727, 0.4909090909090909, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4909090909090909, 0.45454545454545453, 0.5636363636363636]
%Pbar = 0.779929161747
%PbarE = 0.551184888955
%kappa = 0.50966259193
%percentage true 0.688311688312
%


% euro subClassOf
% 7 raters.
% 20 subjects.
% 2 categories.
% p = [0.2642857142857143, 0.7357142857142858]
% P = [0.5238095238095238, 0.7142857142857143, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.42857142857142855, 0.7142857142857143, 0.7142857142857143, 1.0, 0.7142857142857143, 0.7142857142857143, 1.0, 0.7142857142857143]
% Pbar = 0.761904761905
% PbarE = 0.61112244898
% kappa = 0.387737251815
% percentage true 0.15
% 

%Kappa (with cf_results)
%8 raters.
%20 subjects.
%2 categories.
%p = [0.25, 0.75]
%P = [0.5714285714285714, 0.75, 1.0, 1.0, 0.4642857142857143, 1.0, 1.0, 1.0, 1.0, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.4642857142857143, 0.75, 0.75, 1.0, 0.75, 0.75, 1.0, 0.75]
%Pbar = 0.785714285714
%PbarE = 0.625
%kappa = 0.428571428571
%percentage true 0.15
%Majority check for cf_results
% -----------------------------------> always of majority opinion



\textbf{Plugin Usability} was assessed by asking the three ontology engineers to answer to three questions on a scale from 0 (completely disagree) to 5 (completely agree), as follows:

\begin{itemize}
\item The plugin functionality was easy to use. [5, 4, 4]. 
\item I would prefer to use the plugin as opposed to doing these tasks manually. [5, 5, 5]
\item The use of the plugin saves a lot of time to the ontology engineer. [5, 5, 5]
\end{itemize}

From free-form comments, the evaluators considered the recursive task verification as very useful in their work as it would allow them to easily verify large parts of the ontology. Given the small scale of the usability evaluation, we consider it as only indicative at this stage of the fact that the Plugin has a good usability. One user suggested to make the plugin data and results more visually
appealing, as well as showing the anticipated sum of cost before sending the task to CrowdFlower -- both of which is under development. 
A larger usability study is planned as future work.

%\begin{verbatim}
%1)The documentation was easy to understand. 
%-> 4, th part about installing plug-in was a bit of mixture with general info about protege and the installation itself.
%2) The plugin functionality was easy to use.
%-> 5 , it is especially good that one can choose to make the check for all ontology at once by choosing the "check the subtree".
%3) I would prefer to use the plugin as opposed to doing these tasks manually.
%-> 5
%4) The use of the plugin saves a lot of time to the ontology engineer. 
%-> 5
%\end{verbatim}

\section{Conclusions and Future Work}

FW:

1) We need clear methodologies about how to embed crowdsourcing in OE, similarly to the "annotation science" in NLP

2) new plugin version will allow greater control over job settings as well as monitoring the results as they become available from within Prot\`eg\`e.

3) usability study



\bibliographystyle{plain}
\bibliography{cl-iswc14}


\end{document}
